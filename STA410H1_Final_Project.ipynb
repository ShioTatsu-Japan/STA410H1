{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!rm -rf my_package\n",
        "!mkdir my_package"
      ],
      "metadata": {
        "id": "Vuz9-I7iEa8F"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile my_package/__init__.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pP8eAF_z08l-",
        "outputId": "0a0e6a52-fb98-422b-e6c2-94f9fe8ebb7f"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing my_package/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile my_package/final_project.py\n",
        "\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import sobol_seq\n",
        "import multiprocessing as mp\n",
        "\n",
        "random.seed(410)\n",
        "\n",
        "\n",
        "# Section 2\n",
        "\n",
        "## Section 2.1\n",
        "def leibniz_pi(iterations):\n",
        "    pi_est = 0.0\n",
        "    for n in range(iterations):\n",
        "        pi_est += ((-1)**n) / (2*n + 1)\n",
        "    return 4 * pi_est\n",
        "\n",
        "## Section 2.2\n",
        "def integrand(x):\n",
        "    \"\"\"Compute sqrt(1 - x^2), the integrand corresponding to the quarter circle.\"\"\"\n",
        "    return (1 - x**2) ** 0.5\n",
        "\n",
        "def trapezoidal_integration(a, b, N):\n",
        "    \"\"\"\n",
        "    Approximate the integral of f(x) from x=a to x=b using the trapezoidal rule with N subintervals.\n",
        "    Returns 4 * (approximate integral) to estimate π.\n",
        "    \"\"\"\n",
        "    h = (b - a) / N\n",
        "    s = 0.5 * (integrand(a) + integrand(b))\n",
        "    for k in range(1, N):\n",
        "        s += integrand(a + k * h)\n",
        "    return 4 * s * h  # Multiply by 4 to obtain π\n",
        "\n",
        "## Section 2.3\n",
        "def monte_carlo_pi(samples):\n",
        "    \"\"\"Estimate π using Monte Carlo by sampling 'samples' points in [0,1]x[0,1].\"\"\"\n",
        "    inside_circle = 0\n",
        "    for _ in range(samples):\n",
        "        x = random.random()\n",
        "        y = random.random()\n",
        "        if x**2 + y**2 <= 1:\n",
        "            inside_circle += 1\n",
        "    return 4 * (inside_circle / samples)\n",
        "\n",
        "## Section 2.4\n",
        "def mcmc_pi(samples, step_size=0.1):\n",
        "    \"\"\"\n",
        "    Estimate π using a simple random-walk MCMC approach within the unit square.\n",
        "\n",
        "    Parameters:\n",
        "    samples   (int) : Number of MCMC steps to perform\n",
        "    step_size (float): Max step size in both x and y directions for each proposal\n",
        "\n",
        "    Returns:\n",
        "    float: The estimated value of π after 'samples' MCMC steps.\n",
        "    \"\"\"\n",
        "    # Initialize the chain at the center of the unit square (0.5, 0.5)\n",
        "    x, y = 0.5, 0.5\n",
        "    inside_circle = 0\n",
        "\n",
        "    for _ in range(samples):\n",
        "        # Propose new point\n",
        "        x_new = x + random.uniform(-step_size, step_size)\n",
        "        y_new = y + random.uniform(-step_size, step_size)\n",
        "\n",
        "        # Check if new point is within bounds\n",
        "        if 0 <= x_new <= 1 and 0 <= y_new <= 1:\n",
        "            # Accept the new point\n",
        "            x, y = x_new, y_new\n",
        "        # Count if point is inside quarter-circle\n",
        "        if x**2 + y**2 <= 1:\n",
        "            inside_circle += 1\n",
        "\n",
        "    return 4 * (inside_circle / samples)\n",
        "\n",
        "\n",
        "# Section 3\n",
        "## Section 3.1.1\n",
        "def leibniz_pi_kahan(iterations):\n",
        "    \"\"\"\n",
        "    Compute the Leibniz series approximation of π using Kahan summation\n",
        "    to reduce floating-point round-off error.\n",
        "\n",
        "    Parameters:\n",
        "    iterations (int): Number of terms to sum in the Leibniz series\n",
        "\n",
        "    Returns:\n",
        "    float: The approximation of π after 'iterations' terms\n",
        "    \"\"\"\n",
        "    pi_est = 0.0\n",
        "    c = 0.0  # compensation\n",
        "    for n in range(iterations):\n",
        "        # term = ((-1)**n) / (2*n + 1)\n",
        "        term = (1.0 if (n % 2 == 0) else -1.0) / (2*n + 1)\n",
        "\n",
        "        # Kahan Summation\n",
        "        y = term - c\n",
        "        t = pi_est + y\n",
        "        c = (t - pi_est) - y\n",
        "        pi_est = t\n",
        "\n",
        "    return 4 * pi_est\n",
        "\n",
        "def leibniz_partial_sum(N):\n",
        "    \"\"\"Compute the partial sum S_N of the Leibniz series for π/4.\"\"\"\n",
        "    s = 0.0\n",
        "    for n in range(N):\n",
        "        s += ((-1)**n)/(2*n+1)\n",
        "    return s\n",
        "\n",
        "## Section 3.1.2\n",
        "def aitken(N):\n",
        "    \"\"\"Apply one step of Aitken's Δ² acceleration to the Leibniz series partial sums.\"\"\"\n",
        "    S0 = leibniz_partial_sum(N)\n",
        "    S1 = leibniz_partial_sum(N+1)\n",
        "    S2 = leibniz_partial_sum(N+2)\n",
        "\n",
        "    d0 = S1 - S0  # ΔS0\n",
        "    d1 = S2 - S1  # ΔS1\n",
        "    d2 = d1 - d0  # Δ^2S0\n",
        "\n",
        "    # Aitken’s Formula\n",
        "    # S0* = S0 - (d0^2 / d2)\n",
        "    if abs(d2) < 1e-18:\n",
        "        # Avoid division by zero in degenerate cases\n",
        "        S0_star = S2\n",
        "    else:\n",
        "        S0_star = S0 - (d0 * d0 / d2)\n",
        "\n",
        "    return 4.0 * S0_star  # multiply by 4 for pi\n",
        "\n",
        "## Section 3.2.1\n",
        "def simpsons_rule_integration(a, b, N):\n",
        "    # N must be even\n",
        "    if N % 2 != 0:\n",
        "        N += 1\n",
        "\n",
        "    h = (b - a) / N\n",
        "    s = integrand(a) + integrand(b)\n",
        "\n",
        "    # Accumulate terms with alternating factors of 4 and 2\n",
        "    for k in range(1, N):\n",
        "        xk = a + k*h\n",
        "        if k % 2 == 1:\n",
        "            s += 4 * integrand(xk)\n",
        "        else:\n",
        "            s += 2 * integrand(xk)\n",
        "\n",
        "    return (h/3.0) * s\n",
        "\n",
        "## Section 3.2.2\n",
        "def monte_carlo_integration(num_samples):\n",
        "    count_inside = 0\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        x = random.random()\n",
        "        y = random.random()\n",
        "\n",
        "        # Check if point is under quarter circle\n",
        "        if x*x + y*y <= 1.0:\n",
        "            count_inside += 1\n",
        "\n",
        "    return 4.0 * count_inside / num_samples\n",
        "\n",
        "## Section 3.3.1\n",
        "def stratified_sampling_pi(strata, samples_per_stratum):\n",
        "    inside_circle = 0\n",
        "    total_samples = strata * strata * samples_per_stratum\n",
        "\n",
        "    # size of each sub-square\n",
        "    step = 1.0 / strata\n",
        "\n",
        "    for i in range(strata):\n",
        "        for j in range(strata):\n",
        "            for _ in range(samples_per_stratum):\n",
        "                # pick random point inside [i*step, (i+1)*step] x [j*step, (j+1)*step]\n",
        "                x = (i + random.random()) * step\n",
        "                y = (j + random.random()) * step\n",
        "\n",
        "                if x*x + y*y <= 1.0:\n",
        "                    inside_circle += 1\n",
        "\n",
        "    return 4.0 * inside_circle / total_samples\n",
        "\n",
        "## Section 3.3.2\n",
        "def quasi_monte_carlo_pi(num_samples):\n",
        "    inside_circle = 0\n",
        "    # Generate sobol points in 2 dimensions\n",
        "    points = sobol_seq.i4_sobol_generate(2, num_samples)\n",
        "    for (x, y) in points:\n",
        "        if x*x + y*y <= 1.0:\n",
        "            inside_circle += 1\n",
        "    return 4.0 * inside_circle / num_samples\n",
        "\n",
        "## Section 3.3.3\n",
        "def sample_mc_3d(num_points):\n",
        "    \"\"\"\n",
        "    Estimate the value of π using a Monte Carlo method by sampling points\n",
        "    inside a cube [-1, 1]^3 and checking how many lie within the unit sphere.\n",
        "\n",
        "    :param num_points: Number of random points to sample.\n",
        "    :return: Approximate value of π.\n",
        "    \"\"\"\n",
        "    count_inside = 0\n",
        "    for _ in range(num_points):\n",
        "        x = random.uniform(-1, 1)\n",
        "        y = random.uniform(-1, 1)\n",
        "        z = random.uniform(-1, 1)\n",
        "        if x*x + y*y + z*z <= 1:\n",
        "            count_inside += 1\n",
        "\n",
        "    fraction = count_inside / num_points\n",
        "    pi_estimate = 6 * fraction\n",
        "    return pi_estimate\n",
        "\n",
        "## Section 3.3.4\n",
        "def sample_gaussian_2d(sigma=0.5):\n",
        "    \"\"\"\n",
        "    Draw a single sample (X, Y) from a 2D Gaussian with mean (0,0) and\n",
        "    diagonal covariance matrix sigma^2 * I.\n",
        "\n",
        "    Using the Box-Muller transform for two standard normals, then scale by sigma.\n",
        "    \"\"\"\n",
        "    u1 = random.random()\n",
        "    u2 = random.random()\n",
        "    # Standard normal using Box-Muller\n",
        "    r = math.sqrt(-2.0 * math.log(u1))\n",
        "    theta = 2.0 * math.pi * u2\n",
        "    z1 = r * math.cos(theta)\n",
        "    z2 = r * math.sin(theta)\n",
        "    # Scale by sigma\n",
        "    return sigma * z1, sigma * z2\n",
        "\n",
        "def gauss_pdf(x, y, sigma=0.5):\n",
        "    \"\"\"\n",
        "    The value of the 2D Gaussian PDF with mean (0,0) and variance sigma^2 (isotropic).\n",
        "    q(x,y) = (1 / (2*pi*sigma^2)) * exp(-(x^2 + y^2)/(2*sigma^2))\n",
        "    \"\"\"\n",
        "    coeff = 1.0 / (2.0 * math.pi * sigma * sigma)\n",
        "    exponent = - (x*x + y*y) / (2.0 * sigma*sigma)\n",
        "    return coeff * math.exp(exponent)\n",
        "\n",
        "def estimate_pi_importance_gaussian(N, sigma=0.5):\n",
        "    \"\"\"\n",
        "    Estimate pi using Importance Sampling with a 2D Gaussian proposal distribution.\n",
        "\n",
        "    Steps:\n",
        "      1. Generate N samples (x,y) ~ Gaussian(0, sigma^2 I)\n",
        "      2. For each sample, compute the indicator if it's inside the unit circle\n",
        "         and the importance weight w = 1/q(x,y)\n",
        "      3. Sum up weights for those inside the circle\n",
        "      4. pi ~ 4 * (average of those weights)\n",
        "    \"\"\"\n",
        "    w_sum = 0.0\n",
        "    for _ in range(N):\n",
        "        x, y = sample_gaussian_2d(sigma)\n",
        "        # Check if inside unit circle\n",
        "        inside = (x*x + y*y <= 1.0)\n",
        "        if inside:\n",
        "            # Importance weight = 1 / q(x,y)\n",
        "            q_val = gauss_pdf(x, y, sigma)\n",
        "            # If q_val is extremely small, watch out for floating-point overflow in 1/q_val\n",
        "            w_sum += 1.0 / q_val\n",
        "        # If not inside, weight = 0 (i.e. does not contribute)\n",
        "\n",
        "    pi_est = (w_sum / N)\n",
        "    return pi_est\n",
        "\n",
        "## Section 3.4.1\n",
        "def mcmc_pi_adaptive(samples, initial_step=0.1, target_accept=0.3, adapt_interval=1000):\n",
        "    \"\"\"\n",
        "    MCMC approach to estimate π in the unit square [0,1] x [0,1].\n",
        "    Adaptively adjust the step size to maintain a target acceptance rate.\n",
        "\n",
        "    :param samples: total number of MCMC steps\n",
        "    :param initial_step: initial step size for x, y moves\n",
        "    :param target_accept: target acceptance ratio (e.g. 0.3 ~ 0.5)\n",
        "    :param adapt_interval: how often (in steps) we recalculate acceptance and adapt step size\n",
        "    :return: approximate value of π\n",
        "    \"\"\"\n",
        "    x, y = 0.5, 0.5  # start near the center\n",
        "    step_size = initial_step\n",
        "\n",
        "    inside_circle = 0\n",
        "    accepted = 0  # count how many proposals are accepted\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for i in range(1, samples + 1):\n",
        "        # Propose new point\n",
        "        x_new = x + random.uniform(-step_size, step_size)\n",
        "        y_new = y + random.uniform(-step_size, step_size)\n",
        "\n",
        "        # Metropolis acceptance if in [0,1]^2\n",
        "        if 0 <= x_new <= 1 and 0 <= y_new <= 1:\n",
        "            x, y = x_new, y_new\n",
        "            accepted += 1\n",
        "\n",
        "        # Check if inside quarter circle\n",
        "        if x*x + y*y <= 1.0:\n",
        "            inside_circle += 1\n",
        "\n",
        "        # Every adapt_interval steps, adjust step_size\n",
        "        if i % adapt_interval == 0:\n",
        "            acc_rate = accepted / adapt_interval\n",
        "            # If acceptance > target, increase step; else decrease step\n",
        "            if acc_rate > target_accept:\n",
        "                step_size *= 1.1  # enlarge step\n",
        "            else:\n",
        "                step_size *= 0.9  # shrink step\n",
        "            accepted = 0  # reset accepted count\n",
        "\n",
        "    pi_estimate = 4.0 * inside_circle / samples\n",
        "    return pi_estimate\n",
        "\n",
        "## Section 3.4.2\n",
        "def mcmc_chain(samples, step_size=0.1, seed=None):\n",
        "    \"\"\"\n",
        "    Single-chain MCMC for pi estimation. Returns the number of points inside circle.\n",
        "    We separate 'inside' count from total so we can combine results later.\n",
        "    \"\"\"\n",
        "    if seed is not None:\n",
        "        random.seed(seed)\n",
        "\n",
        "    x, y = 0.5, 0.5\n",
        "    inside_circle = 0\n",
        "\n",
        "    for _ in range(samples):\n",
        "        x_new = x + random.uniform(-step_size, step_size)\n",
        "        y_new = y + random.uniform(-step_size, step_size)\n",
        "\n",
        "        if 0 <= x_new <= 1 and 0 <= y_new <= 1:\n",
        "            x, y = x_new, y_new\n",
        "\n",
        "        if x*x + y*y <= 1.0:\n",
        "            inside_circle += 1\n",
        "\n",
        "    return inside_circle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zg1Up9VxG4gx",
        "outputId": "93822510-3a52-484c-a366-c1b775bbfafa"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing my_package/final_project.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile setup.py\n",
        "from setuptools import setup, find_packages\n",
        "\n",
        "setup(\n",
        "    name='my_package',\n",
        "    version='0.1.0',\n",
        "    packages=find_packages(),\n",
        "    install_requires=[\n",
        "        'sobol_seq',  # 外部ライブラリだけ\n",
        "    ],\n",
        "    author='Tatsuya Shiokawa',\n",
        "    author_email='tatsuya.shiokawa@mail.utoronto.ca',\n",
        "    description='Final Project',\n",
        "    url='https://github.com/ShioTatsu-Japan/STA410H1',\n",
        "    classifiers=[\n",
        "        'Programming Language :: Python :: 3',\n",
        "        'License :: OSI Approved :: MIT License',\n",
        "    ],\n",
        "    python_requires='>=3.6',\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlMyEXUuG8T-",
        "outputId": "4199ae82-11e5-4994-f172-75b9eb656073"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7p8Oe3oEiTi",
        "outputId": "dc93de3a-a9f6-4fbe-aca3-1089c9871458"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sobol_seq in /usr/local/lib/python3.11/dist-packages (from my_package==0.1.0) (0.2.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sobol_seq->my_package==0.1.0) (1.14.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from sobol_seq->my_package==0.1.0) (2.0.2)\n",
            "Installing collected packages: my_package\n",
            "  Attempting uninstall: my_package\n",
            "    Found existing installation: my_package 0.1.0\n",
            "    Uninstalling my_package-0.1.0:\n",
            "      Successfully uninstalled my_package-0.1.0\n",
            "  Running setup.py develop for my_package\n",
            "Successfully installed my_package-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from my_package.final_project import leibniz_pi, integrand, trapezoidal_integration, \\\n",
        "monte_carlo_pi, mcmc_pi, leibniz_pi_kahan, leibniz_partial_sum, aitken, \\\n",
        "simpsons_rule_integration, monte_carlo_integration, stratified_sampling_pi, \\\n",
        "quasi_monte_carlo_pi, sample_mc_3d, sample_gaussian_2d, gauss_pdf, \\\n",
        "estimate_pi_importance_gaussian, mcmc_pi_adaptive, mcmc_chain\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import sobol_seq\n",
        "import multiprocessing as mp"
      ],
      "metadata": {
        "id": "-DyF640NEu5u"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Table of Contents\n",
        "\n",
        "### 1. Introduction and Motivation\n",
        "\n",
        "### 2. Four Methods for Estimating $\\pi$\n",
        "  * #### 2.1. Leibniz Series\n",
        "  * #### 2.2. Integral-Based Estimation\n",
        "  * #### 2.3. Classic Monte Carlo\n",
        "  * #### 2.4. Markov Chain Monte Carlo (MCMC)\n",
        "\n",
        "### 3. Advanced Techniques and Performance Analysis\n",
        "  * #### 3.1. Advanced Techniques for Leibniz Series\n",
        "    * 3.1.1. Kahan Summation\n",
        "    * 3.1.2. Aitken Formula\n",
        "  * #### 3.2. Advanced Techniques for Integral-Based Estimation\n",
        "    * 3.2.1. Simpson's Rule\n",
        "    * 3.2.2. Monte Carlo Integration\n",
        "  * #### 3.3. Advanced Techniques for Classic Monte Carlo\n",
        "    * 3.3.1. Stratified Sampling\n",
        "    * 3.3.2. Quasi Monte Carlo\n",
        "    * 3.3.3. 3D Monte Carlo\n",
        "    * 3.3.4. 2D Gaussian\n",
        "  * #### 3.4. Advanced Techniques for MCMC\n",
        "    * 3.4.1. Adaptive Step-Size MCMC\n",
        "    * 3.4.2. Parallel Chains\n",
        "\n",
        "### 4. Summary\n",
        "\n",
        "### 5. Reference"
      ],
      "metadata": {
        "id": "3myMuJ1R3YIt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction and Motivation\n",
        "\n",
        "* **Background:** $\\pi$ is one of the most famous constants in mathematics – defined as the ratio of a circle’s circumference to its diameter. This number is irrational, meaning it cannot be expressed as a simple fraction and its decimal expansion goes on forever without repeating. $\\pi$ appears throughout mathematics, science, and engineering: it shows up in geometric formulas (circle area and volume of a sphere), in physics equations (e.g. Einstein’s field equations in general relativity and wave mechanics), and even in everyday technologies like GPS and electronics​. Because $\\pi$ is a transcendental number with infinitely many non-repeating decimals, humanity has long been fascinated with determining its value as accurately as possible. From ancient civilizations’ rough estimates to Archimedes’ polygon approximation method and modern computer algorithms, the quest to accurately estimate $\\pi$ has been a driving challenge and a benchmark for computational techniques​​.\n",
        "\n",
        "* **Motivation:** Estimating $\\pi$ is an interesting and useful problem because it connects multiple areas of computational mathematics. There are many different ways to arrive at $\\pi$, each illuminating a different concept. **Infinite series** provide one approach: for example, the Leibniz series (also known as the Gregory-Leibniz series) expresses $\\pi$ as an infinite alternating sum of fractions. **Numerical integration** offers another route by evaluating integrals that equal $\\pi$ – for instance, using calculus to compute the area under a curve or the area of a quarter-circle. **Statistical methods** demonstrate a completely different perspective: techniques like Monte Carlo simulation use randomness to approximate $\\pi$. In a Monte Carlo approach, one can “throw darts” at a square and see what fraction land inside an inscribed quarter-circle to estimate $\\pi$. This blend of methods – analytical series, deterministic calculus, and random simulation – makes $\\pi$ estimation a rich educational example that ties together theory and practice across disciplines."
      ],
      "metadata": {
        "id": "SkKzDICUxEHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Four Methods for Estimating $\\pi$"
      ],
      "metadata": {
        "id": "Rnt3iTklywsX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. Leibniz Series\n",
        "\n",
        "The Leibniz formula for $\\pi$ is an infinite series expression for $\\pi$. It states that:\n",
        "\n",
        "$$\n",
        "\\frac{\\pi}{4} = 1 - \\frac{1}{3} + \\frac{1}{5} - \\frac{1}{7} + \\frac{1}{9} + \\cdots = \\sum_{k=0}^\\infty \\frac{(-1)^k}{2k+1}\n",
        "$$\n",
        "\n",
        "The terms in this series decrease relatively slowly, which makes convergence to $\\pi$ quite gradual. Achieving high precision requires summing a large number of terms. The alternating nature $(-1)^n$ ensures that partial sums oscillate around the true value of $\\pi$. By the Alternating Series Test, the error after $N$ terms is bounded by the magnitude of the $(N+1)$-th term.\n"
      ],
      "metadata": {
        "id": "DoV3M8U0zu-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    iterations = 10_000_000\n",
        "    start = time.time()\n",
        "    pi_approx = leibniz_pi(iterations)\n",
        "    end = time.time()\n",
        "\n",
        "    print(f\"Leibniz Series Approximation of π: {pi_approx}\")\n",
        "    print(f\"Absolute Error: {abs(pi_approx - math.pi)}\")\n",
        "    print(f\"Time Taken: {end - start} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDPGFnw-8IWG",
        "outputId": "b1a424c7-37ab-423e-dc3b-6aaacb8e8f25"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Leibniz Series Approximation of π: 3.1415925535897915\n",
            "Absolute Error: 1.0000000161269895e-07\n",
            "Time Taken: 6.288283586502075 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. Integral-Based Estimation\n",
        "\n",
        "Numerical integration provides a deterministic way to approximate definite integrals whose exact values may correspond to $\\pi$. One classic example is:\n",
        "\n",
        "$$\n",
        "\\frac{\\pi}{4} = \\int_0^1\\sqrt{1-x^2}\\,dx\n",
        "$$\n",
        "\n",
        "which effectively computes the area of a quarter-circle of radius 1. To approximate the integral, we can use **trapezoidal rule**. Each subinterval is treated as a trapezoid rather than a rectangle. If $f(x)$ is the function to integrate, then the sum of trapezoidal areas can be more accurate for smooth functions.\n",
        "\n",
        "$$\n",
        "\\int_a^b f(x) \\, dx \\approx \\frac{h}{2}\\left[f(x_0) + 2\\sum_{k=1}^{N-1}f(x_k) + f(x_N)\\right]\n",
        "$$\n",
        "\n",
        "where $h = \\frac{b-a}{2}$."
      ],
      "metadata": {
        "id": "hXbISpT5zx_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    N = 10_000_000\n",
        "    start = time.time()\n",
        "    pi_approx = trapezoidal_integration(0, 1, N)\n",
        "    end = time.time()\n",
        "\n",
        "    print(f\"Trapezoidal Rule Approximation of π: {pi_approx}\")\n",
        "    print(f\"Absolute Error: {abs(pi_approx - math.pi)}\")\n",
        "    print(f\"Time Taken: {end - start} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIuO6Alc_qZX",
        "outputId": "2553b3d0-8e62-4e98-c205-3aba2d4bb32f"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trapezoidal Rule Approximation of π: 3.1415926535527117\n",
            "Absolute Error: 3.708144902248023e-11\n",
            "Time Taken: 7.322698354721069 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3. Classic Monte Carlo\n",
        "\n",
        "The **Monte Carlo Rejection Sampling** method uses random uniform sampling to estimate areas by comparing a target area to a known reference area. In this case, we draw points uniformly in the unit square $[0,1]\\times[0,1]$ and check whether each point falls inside the quarter of a unit circle drawn within the square. The quarter circle of radius 1 has area $\\pi r^2 / 4 = \\pi/4$ (since $r=1$), which is exactly $\\pi/4$ of the unit square’s area. Because points are uniformly distributed, the probability that a random point lies inside the quarter circle is equal to the area of the quarter circle divided by the area of the square (i.e. $\\pi/4$)​. The underlying reason for this method converging $\\pi$ is the **Law of Large Numbers**. Each random point can be thought of as a Bernoulli trial that “succeeds” if it lands inside the quarter circle (with success probability $p = \\pi/4$) and “fails” otherwise. When we sample many points, the proportion of successes $N_{\\text{inside}}/N$ will converge to the true success probability $p$ in the long run​."
      ],
      "metadata": {
        "id": "22tptWlTz1-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    samples = 10_000_000\n",
        "    start = time.time()\n",
        "    pi_approx = monte_carlo_pi(samples)\n",
        "    end = time.time()\n",
        "\n",
        "    print(f\"Monte Carlo Approximation of π: {pi_approx}\")\n",
        "    print(f\"Absolute Error: {abs(pi_approx - math.pi)}\")\n",
        "    print(f\"Time Taken: {end - start} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMXCpucGEjE2",
        "outputId": "36bf184c-49ac-42b3-c8d8-25ad9ec14d37"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Monte Carlo Approximation of π: 3.1424308\n",
            "Absolute Error: 0.0008381464102069636\n",
            "Time Taken: 4.666754961013794 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4. Markov Chain Monte Carlo (MCMC)\n",
        "\n",
        "In traditional Monte Carlo approaches, we sample points $(x,y)$ independently and uniformly in the unit square $[0,1] \\times [0,1]$. By counting how many fall in the quarter-circle $x^2 + y^2 \\le 1$, we estimate the ratio of areas and thus approximate $\\pi$ (because the quarter-circle has area $\\pi/4$). However, **MCMC** replaces the idea of “sampling points independently” with a stochastic process that moves from one point $(x,y)$ to another in a dependent manner, forming a Markov chain."
      ],
      "metadata": {
        "id": "cOjZsbIGz4NP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    samples = 10_000_000\n",
        "    start = time.time()\n",
        "    pi_approx = mcmc_pi(samples)\n",
        "    end = time.time()\n",
        "\n",
        "    print(f\"MCMC Approximation of π: {pi_approx}\")\n",
        "    print(f\"Absolute Error: {abs(pi_approx - math.pi)}\")\n",
        "    print(f\"Time Taken: {end - start} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkxmsFjXFPvz",
        "outputId": "ed99a8cb-4785-4a49-c5af-7314e5c4167b"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MCMC Approximation of π: 3.139588\n",
            "Absolute Error: 0.0020046535897932927\n",
            "Time Taken: 8.487161874771118 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Advanced Techniques and Performance Analysis"
      ],
      "metadata": {
        "id": "2EU0BsmQy1ow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. Advanced Techniques for Leibniz Series"
      ],
      "metadata": {
        "id": "VXa2J0t80LjI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.1. Kahan Summation\n",
        "\n",
        "As previously discussed in 2.1., the Leibniz series converges slowly. Even setting aside the slow convergence, numerical issues also arise from floating-point arithmetic when summing large sequences of positive and negative terms. Round-off errors can accumulate, especially when subtracting nearly equal numbers or adding very small terms to a large running total. A well-known approach, which we also covered in the class, to mitigate round-off error in long summations is **Kahan Summation**. Kahan Summation tracks a “compensation” term to reduce the accumulation of floating-point errors. It is especially helpful in scenarios where a summation has many small terms that can be partially lost when added to a large running total.\n",
        "\n",
        "In standard floating-point addition, if we add a small number $\\delta$ to a much larger number $S$, $\\delta$ might be too small to affect the sum in finite precision. Repeatedly adding small terms leads to growing inaccuracies. Kahan Summation addresses this by maintaining a “compensation” variable $c$ that tracks lost low-order bits in each addition. Each time we add a new term:\n",
        "\n",
        "1. We first subtract the compensation from the current term to produce an adjusted increment $y$.\n",
        "2. We then add $y$ to the running total $\\pi_{\\text{est}}$ but carefully reconstruct the error (the difference between the computed sum and the exact sum), storing it back in $c$.\n",
        "\n",
        "Conceptually, Kahan Summation tries to correct for the small fraction of precision that was “lost” in the previous step, ensuring it is reintroduced in subsequent additions."
      ],
      "metadata": {
        "id": "hL2Sc46t1aNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    iterations = 10_000_000\n",
        "    start = time.time()\n",
        "    pi_approx = leibniz_pi_kahan(iterations)\n",
        "    end = time.time()\n",
        "\n",
        "    print(f\"Leibniz Series Approximation with Kahan summation of π: {pi_approx}\")\n",
        "    print(f\"Absolute Error: {abs(pi_approx - math.pi)}\")\n",
        "    print(f\"Time Taken: {end - start} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dy-G32wAiyuu",
        "outputId": "c70da0de-bc84-4bb0-eadd-e76d878ca964"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Leibniz Series Approximation with Kahan summation of π: 3.1415925535897933\n",
            "Absolute Error: 9.999999983634211e-08\n",
            "Time Taken: 1.9138226509094238 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.2. Aitken Formula\n",
        "\n",
        "**Definition (Aitken Formula):** Given a sequence $\\{s_n\\}_{n\\in\\mathbb{N}}$, define\n",
        "\n",
        "$$\n",
        "\\Delta s_n = s_{n+1} - s_n, \\, \\Delta^2 s_n = \\Delta s_{n+1} \\Delta s_n = s_{n+2} - 2s_{n+1} + s_n\n",
        "$$\n",
        "\n",
        "Aitken's accelerated sequence $\\{s_n^\\star\\}$ is\n",
        "\n",
        "$$\n",
        "s_n^\\star = s_n - \\frac{(\\Delta s_n)^2}{\\Delta^2 s_n} \\, (\\text{assuming $\\Delta^2 s_n \\ne 0$})\n",
        "$$\n",
        "\n",
        "The claim is that if $s_n$ converges to some limit $S$, then $s_n^\\star$ often converges more rapidly to $S$.\n",
        "\n",
        "---\n",
        "\n",
        "This formula assumes that the sequence’s errors behave in a roughly geometric (linear convergence) manner and solves for the sequence’s apparent limit. In fact, if $s_n$ converges linearly with error $s_n - S \\approx C\\rho^n$ for large $n$ (with some constant $C$ and ratio $\\rho$), then Aitken’s process will exactly eliminate the leading $C\\rho^n$ term and give a higher-order error term, thereby dramatically accelerating convergence​.\n",
        "\n",
        "Now, suppose a sequence $\\{s_n\\}_{n\\in\\mathbb{N}}$ can be written as $$s_n = S + A\\lambda^n + (\\text{smaller terms})$$ where $\\lambda$ is some constant. The term $A\\lambda^n$ is the leading contribution to the error $s_n - S$. We can check how $\\Delta^2s_n$ relates to that error term:\n",
        "\n",
        "  1. $$\\Delta s_n = s_{n+1} - s_n = (S + A\\lambda^{n+1}) - (S + A\\lambda^n) = A\\lambda^n(1-\\lambda)$$\n",
        "\n",
        "  2. $$\\Delta^2 s_n = \\Delta s_{n+1} - \\Delta s_n = A\\lambda^{n+1}(1-\\lambda) - A\\lambda^n(1-\\lambda) = A\\lambda^n(1-\\lambda)^2$$\n",
        "\n",
        "Therefore, $$\\frac{(\\Delta s_n)^2}{(\\Delta^2 s_n)} = \\frac{[A\\lambda^n(1-\\lambda)]^2}{A\\lambda^n(1-\\lambda)^2} = A\\lambda^n$$ Hence, $$s_n^\\star = s_n - \\frac{(\\Delta s_n)^2}{\\Delta^2 s_n} = (S + A\\lambda^n) - A\\lambda^n = S + (\\text{smaller terms})$$ The “main” error term $A\\lambda^n$ vanishes in the difference. As a result, $s_n^\\star$ is much closer to $S$ than $s_n$ is, especially when $\\lambda^n$ dominates the tail."
      ],
      "metadata": {
        "id": "ggDuO1TW1ci3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    iterations = 10_000_000\n",
        "    start = time.time()\n",
        "    pi_approx = aitken(iterations)\n",
        "    end = time.time()\n",
        "\n",
        "    print(f\"Leibniz Series Approximation with Aitken of π: {pi_approx}\")\n",
        "    print(f\"Absolute Error: {abs(pi_approx - math.pi)}\")\n",
        "    print(f\"Time Taken: {end - start} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKdWMEheN3ya",
        "outputId": "f18811e3-0be9-4010-af90-01a2c239c3d9"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Leibniz Series Approximation with Aitken of π: 3.1415926535897913\n",
            "Absolute Error: 1.7763568394002505e-15\n",
            "Time Taken: 11.251074314117432 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. Advanced Techniques for Integral-Based Estimation"
      ],
      "metadata": {
        "id": "VWawmkZm0PSM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.1. Simpson's Rule\n",
        "\n",
        "**Simpson’s 1/3 Rule:** Given an interval $[a,b]$, Simpson’s rule approximates the integral by using three sample points: the two endpoints and the midpoint. Let $m = \\frac{a+b}{2}$. We approximate $f(x)$ on $[a,b]$ by a unique quadratic polynomial $P(x)$ that passes through $(a,f(a))$, $(m,f(m))$, and $(b,f(b))$. Integrating this polynomial exactly yields Simpson’s formula:\n",
        "\n",
        "$$\n",
        "\\int_a^b f(x) \\,dx \\approx \\frac{b-a}{6} \\left[f(a) + 4f\\left(\\frac{a+b}{2} \\right) + f(b)\\right]\n",
        "$$\n",
        "\n",
        "This is known as Simpson’s 1/3 rule. The weights $1$, $4$, and $1$ come from the coefficients of the interpolating parabola. Intuitively, the midpoint $f(m)$ is given quadruple weight because the curved path places more “area” around the center of the interval than a straight-line would. This formula can be derived formally using Lagrange polynomial interpolation or by combining simpler rules: in fact, Simpson’s rule can be obtained as a weighted average of the trapezoidal rule and midpoint rule that cancels out lower-order error terms​. (Specifically, one can show $S_{2n} = \\frac{2}{3}M_n + \\frac{1}{3}T_n$, meaning the Simpson result on $2n$ subintervals equals a mix of the midpoint ($M$) and trapezoidal ($T$) results​.) The result is a third-order accurate local approximation that integrates cubics exactly.\n",
        "\n",
        "\n",
        "**Composite Simpson’s Rule:** To approximate $\\int_a^b f(x) \\,dx$ over a larger interval, we apply the above formula on multiple subintervals. Suppose we divide $[a,b]$ into $N$ equal subintervals of width $h = (b-a)/N$. Important: Simpson’s rule requires that $N$ be even, since each quadratic segment spans two subintervals. We then group the subintervals into pairs and apply Simpson’s 1/3 rule on each pair. If $x_0 = a, x_1 = a+h, x_2 = a+2h, \\dots, x_N = b$ are the partition points, the composite Simpson’s rule is:\n",
        "\n",
        "$$\n",
        "\\int_a^b f(x) \\,dx \\approx \\frac{h}{3} \\left[f(x_0) + 4f(x_1) + 2f(x_2) + 4f(x_3) + 2f(x_4) + \\cdots + 4f(x_{N-1}) + 2f(x_N)\\right]\n",
        "$$\n",
        "\n",
        "Here the interior points’ values alternate coefficients $4,2,4,2,\\dots,4$ before ending with $f(x_N)$​. This pattern effectively applies the $1,4,1$ Simpson’s weighting on each pair of subintervals. The composite Simpson formula above assumes $N$ is even​ so that the pattern ends correctly with a $4$ coefficient on the second-to-last point and the last point $f(x_N)$ has coefficient $1$. (For example, if $N=6$, the coefficients would be $1,4,2,4,2,4,1$.) The formula shows that points with odd index (the midpoints of each pair) get weight 4, while the even-index interior points get weight 2 in the summation. This method uses $N+1$ function evaluations in total.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "One of the strength for Composite Simpson's Rule is that if $f$ is sufficiently smooth and has a continuous fourth derivative on $[a,b]$, then the error bound is $$E_\\text{simp} = -\\frac{(b-a)^5}{180N^4}f^{(4)}(\\xi)$$ for some $\\xi \\in [a,b]$. That indicates an error that goes as $O(N^{-4})$. Hence for the same $N$. Simpson’s rule typically gives a much smaller error than the trapezoidal rule (assuming the integrand is nice and smooth). Conversely, to reach a fixed error tolerance, Simpson’s rule can get away with far fewer intervals $N$.\n",
        "\n"
      ],
      "metadata": {
        "id": "2cYWu2pi1jjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    N = 10_000_000  # Large number for high accuracy\n",
        "    start = time.time()\n",
        "    quarter_circle_area = simpsons_rule_integration(0, 1, N)\n",
        "    pi_approx = 4 * quarter_circle_area\n",
        "    end = time.time()\n",
        "\n",
        "    print(f\"Simpson's Rule Approx of pi: {pi_approx}\")\n",
        "    print(f\"Absolute Error: {abs(pi_approx - math.pi)}\")\n",
        "    print(f\"Time Taken: {end - start} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_2m9CMpeWxL",
        "outputId": "f1530cf3-4763-44f9-c1d2-55f6fc1c2430"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simpson's Rule Approx of pi: 3.141592653575458\n",
            "Absolute Error: 1.4335199693960021e-11\n",
            "Time Taken: 3.4363911151885986 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.2. Monte Carlo Integration\n",
        "\n",
        "**Monte Carlo Integration** is a probabilistic method for approximating definite integrals using random sampling. In contrast to traditional deterministic techniques, Monte Carlo methods rely on randomness to estimate an integral’s value, which is especially useful when an integral is analytically intractable or high-dimensional​. Theoretically speaking, Monte Carlo does not converge as quickly as Simpson’s Rule. Monte Carlo has an error on the order of $1/\\sqrt{N}$​, whereas Simpson’s can achieve $1/N^4$ under the right smoothness conditions. That’s a huge difference in convergence rate—Simpson’s is faster to reduce error as $N$ grows. Yet Monte Carlo can often finish sooner (i.e., have “better running time”) in practice. The primary reasons are:\n",
        "\n",
        "1. Trivial Prallelization:\n",
        "  * Each Monte Carlo sample is just $(x,y)$ generated at random, so all samples are independent.\n",
        "  * We can thus throw as many CPU cores or GPUs as we want at the problem.\n",
        "  * This parallelization can be done without worrying about data dependencies or complicated load balancing.\n",
        "\n",
        "2. Each Sample is Very Cheap:\n",
        "  * Checking $x^2 + y^2 \\le 1$ is extremely fast—just two multiplications and one comparison.\n",
        "  * Even at the scale of tens or hundreds of millions of samples, these operations can be done quickly on modern hardware.\n",
        "\n",
        "3. Dimension Independence:\n",
        "  * When you do grid‐based methods (like Simpson’s) in higher dimensions, the number of grid points grows as $N^d$, where $d$ is the dimension $N^d$, where $d$ is the dimension. This often explodes extremely fast as $d$ increases.\n",
        "  * Monte Carlo still maintains an $N^{-1/2}$ convergence even in 10D, 100D, or higher, so you can often handle large‐dimensional problems more feasibly."
      ],
      "metadata": {
        "id": "9ocHZfml1sGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    N = 10_000_000  # Number of random samples\n",
        "    start = time.time()\n",
        "    pi_approx = monte_carlo_integration(N)\n",
        "    end = time.time()\n",
        "\n",
        "    print(f\"Monte Carlo Approx of pi: {pi_approx}\")\n",
        "    print(f\"Absolute Error: {abs(pi_approx - math.pi)}\")\n",
        "    print(f\"Time Taken: {end - start} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd1gX3QyqHgR",
        "outputId": "c1ea5976-f544-4312-d0b2-2a200389c691"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Monte Carlo Approx of pi: 3.1414492\n",
            "Absolute Error: 0.00014345358979328537\n",
            "Time Taken: 1.97788405418396 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3. Advanced Techniques for Classic Monte Carlo"
      ],
      "metadata": {
        "id": "Kk7aMySd0R89"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3.1. Stratified Sampling\n",
        "\n",
        "**Stratified Sampling** introduces a structured approach: the integration domain is partitioned into several disjoint strata, and samples are taken within each stratum rather than from the domain as a whole​. In its simplest form, if the domain is divided into $H$ strata, one allocates a certain number of samples $n_h$ to each stratum $h$ (such that $\\sum_{h=1}^H n_h = N$). Within each stratum, points are sampled (usually uniformly if nothing is known about $f$). These per-stratum estimates are then combined (weighted by the size or probability of each stratum) to produce the overall integral estimate. The key difference from standard Monte Carlo is that each region of the domain is guaranteed to be sampled in stratified sampling, rather than leaving the coverage purely to chance. This method is still unbiased (the expected value is still $I$)​, but by reducing the randomness in how samples are geographically distributed, it can achieve a lower estimator variance than crude Monte Carlo. Stratified sampling is often considered a variance-reduction technique (and sometimes categorized as a quasi-Monte Carlo method, since it introduces deterministic structure into the sampling)​.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#### Mathematical Foundation\n",
        "\n",
        "1. **Estimator Formulation:** If the domain $\\mathcal{X}$ is partitioned into strata $\\mathcal{X}_1, \\mathcal{X}_2, \\dots, \\mathcal{X}_H$, the stratified Monte Carlo estimator can be written as a weighted sum of the averages from each stratum. For example, for a uniform sampling over $\\mathcal{X}$, one can write:\n",
        "$$\n",
        "\\hat{I}_{\\text{strat}} = \\sum_{h=1}^H w_h \\overline{f}_h\n",
        "$$\n",
        "where $w_h = \\frac{|\\mathcal{X}_h|}{|\\mathcal{X}|}$ is the weight of stratum $h$, and $\\overline{f}_h = \\frac{1}{n_h}\\sum{i=1}^{n_h} f(x_h,i)$ is the average of $f$ over the $n_h$ samples taken in stratum $h$. If samples in each stratum are drawn uniformly from that stratum, then $\\mathbb{E}[\\overline{f}_h]$ equals the true average of $f$ over $\\mathcal{X}_h$. It follows that $\\mathbb{E}[\\hat{I}_{\\text{strat}}] = \\sum_h w_h,\\mathbb{E}[\\overline{f}_h] = \\sum_h w_h I_h = I$, where $I_h = \\int{\\mathcal{X}_h} f(x)dx$ is the contribution of stratum $h$ to the total integral​. Thus, $\\hat{I}_{\\text{strat}}$ is an **unbiased** estimator of $I$, just like the standard Monte Carlo estimator.\n",
        "\n",
        "2. **Variance Decomposition:** The real advantage of stratification lies in its effect on the estimator’s variance. Under independent sampling in each stratum, one can show that the variance of the stratified estimator is the sum of the variances of each stratum’s contribution. Mathematically:\n",
        "$$\n",
        "\\text{Var}[\\hat{I}_\\text{strat}] = \\sum_{h=1}^H \\text{Var}\\left(w_h \\overline{f}_h\\right) = \\sum_{h=1}^H w_h^2 \\frac{\\sigma_h^2}{n_h}\n",
        "$$\n",
        "where $\\sigma_h^2$ is the variance of $f(X)$ restricted to stratum $h$​. Compare this to the variance of the crude Monte Carlo estimator: $\\mathrm{Var}[\\hat{I}_\\text{MC}] = \\frac{\\sigma^2}{N}$, where $\\sigma^2 = \\mathrm{Var}(f(X))$ over the whole domain​. We can relate $\\sigma^2$ to the stratification by the **Law of Total Variance**: $\\sigma^2 = \\sum_{h} w_h,\\sigma_h^2 + \\sum_h w_h(\\mu_h - I)^2$, where $\\mu_h$ is the true mean of $f$ in stratum $h$​. The second term $\\sum_h w_h(\\mu_h - I)^2$ represents the variance between strata (due to different strata having different means).　In a well-executed stratified sampling (especially if we allocate samples proportional to strata sizes), the between-stratum variance is effectively removed from the estimator’s variance. The stratified estimator’s variance becomes $\\frac{1}{N}\\sum_h w_h,\\sigma_h^2$, omitting the between-stratum term​. This is always less than or equal to the crude variance $\\frac{\\sigma^2}{N}$, since $\\sigma^2$ included the extra nonnegative term for between-stratum differences​. In the best case where strata are chosen such that $f$ is nearly constant within each stratum (so that each $\\sigma_h^2$ is very low), the variance reduction is significant. In the worst case where the function’s average is the same across all strata (so that dividing was pointless), one finds $\\mu_h \\approx I$ for all $h$ and the between-stratum term is zero – in that degenerate case stratification offers no variance improvement​. Importantly, under a reasonable allocation of samples to strata, stratified sampling never increases the variance compared to standard Monte Carlo; it will reduce variance unless the stratification did nothing (i.e. all strata had identical means)​.\n"
      ],
      "metadata": {
        "id": "XK1zEC6r14U9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    strata = 1000\n",
        "    samples_per_stratum = 10  # total = 1000*1000*10 = 10 million\n",
        "    start = time.time()\n",
        "    pi_approx = stratified_sampling_pi(strata, samples_per_stratum)\n",
        "    end = time.time()\n",
        "\n",
        "    print(f\"Stratified Approx: {pi_approx}\")\n",
        "    print(f\"Absolute Error: {abs(pi_approx - math.pi)}\")\n",
        "    print(f\"Time: {end - start} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltilfRiBrakh",
        "outputId": "051209d5-3ccc-4b4a-cc3c-9a9d0e14496b"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stratified Approx: 3.1415844\n",
            "Absolute Error: 8.253589792950322e-06\n",
            "Time: 3.7566256523132324 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3.2. Quasi Monte Carlo\n",
        "\n",
        "**Quasi–Monte Carlo** uses deterministic low-discrepancy sequences for choosing sample points instead of random draws. The term “quasi” highlights that QMC points are not truly random, but they approximate the uniform coverage one would expect from random points in the limit. QMC replaces randomness with carefully chosen low-discrepancy sequences that spread out points more uniformly over the integration domain. If $x_1,\\dots,x_N$ are selected from a low-discrepancy sequence, we still approximate $I \\approx \\frac{1}{N}\\sum_{i=1}^N f(x_i)$, but we expect a smaller error than a random Monte Carlo sample of the same size. A classic theoretical result called the Koksma–Hlawka inequality formalizes this: for a function $f$ with bounded variation $V(f)$ (in the sense of Hardy–Krause) and points ${x_i}$, the QMC error is bounded by\n",
        "$$\n",
        "\\left|\\int_{\\Omega} f(x) \\,dx - \\frac{1}{N}\\sum_{i=1}^N f(x_i)\\right| \\le V(f) \\times D_N^\\star\n",
        "$$\n",
        "where $D_N^\\star$ is the star-discrepancy of the point set. The star-discrepancy $D_N^\\star$ measures how far the empirical distribution of points ${x_i}$ deviates from the uniform distribution over $\\Omega$. Intuitively, low discrepancy means the points “fill” the domain evenly: any subregion of the domain contains roughly the proportional number of points that it should contain​. Monte Carlo points achieve low discrepancy only on average (with high probability as $N$ grows), whereas QMC sequences are designed to have provably low discrepancy for every $N$.\n",
        "\n",
        "The main reason QMC often outperforms standard Monte Carlo is the reduction of discrepancy (i.e. improved uniformity) which leads to lower integration error. Monte Carlo’s $1/\\sqrt{N}$ error arises from the randomness – points can bunch together or leave large regions unsampled, causing statistical fluctuations. Quasi-random points avoid this. They “cover the domain of interest quickly and evenly”, which means every region gets its fair share of sample points (proportional to its volume) more strictly than with random sampling. This even coverage reduces variance because it prevents the scenario where, by chance, many points fall in an area where $f(x)$ is unusually high or low. In effect, QMC behaves like an extreme form of stratified sampling: the sample points are so uniformly dispersed that they act as if the domain were partitioned into many tiny subregions (strata) with one point in each. Another perspective comes from the law of large numbers and equidistribution: any set of truly random points will eventually cover the domain uniformly on average, but with random fluctuations. Low-discrepancy sequences aim to enforce uniform coverage at every finite $N$, thereby minimizing the worst-case error in approximation. The Koksma–Hlawka inequality mentioned earlier quantifies this: the integration error is at most the product of the function’s variation and the sequence’s discrepancy. For a smooth function (finite variation), using points with discrepancy $D_N$ can drastically shrink the error bound. For example, if $D_N$ is on the order of $(\\log N)^s/N$, the bound suggests an error roughly $O((\\log N)^s/N)$, which for large $N$ is much smaller than the $O(1/\\sqrt{N})$ of Monte Carlo. Even though this worst-case bound can be loose in practice, it reflects the potential of QMC to achieve nearly $O(1/N)$ convergence (up to logarithmic factors) for well-behaved integrands in fixed dimension.\n"
      ],
      "metadata": {
        "id": "F91-LQmG18Ga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    N = 10_000_000\n",
        "    start = time.time()\n",
        "    pi_approx = quasi_monte_carlo_pi(N)\n",
        "    end = time.time()\n",
        "\n",
        "    print(f\"QMC (Sobol) Approx of pi: {pi_approx}\")\n",
        "    print(f\"Absolute Error: {abs(pi_approx - math.pi)}\")\n",
        "    print(f\"Time: {end - start} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nr-LwsCY53DI",
        "outputId": "af4f4213-1a36-4604-d3bf-674a95bdb002"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QMC (Sobol) Approx of pi: 3.1415952\n",
            "Absolute Error: 2.5464102066941052e-06\n",
            "Time: 134.91256976127625 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3.3. 3D Monte Carlo\n",
        "\n",
        "Consider a unit sphere of radius 1 centered at the origin in 3-dimensional space. This sphere is inscribed in a cube (also centered at the origin) with side length 2 (extending from -1 to 1 along each axis). The volume of the cube is $V_\\text{cube} = 8$. The volume of the unit sphere (of radius $r=1$) is known to be:\n",
        "$$\n",
        "V_{\\text{sphere}} = \\frac{4}{3}\\pi r^3 = \\frac{4}{3}\\pi\n",
        "$$\n",
        "Now, if one samples a point uniformly at random in the cube $[-1,1]\\times[-1,1]\\times[-1,1]$, the probability of it landing inside the sphere is equal to the volume fraction of the sphere relative to the cube. In other words,\n",
        "$$\n",
        "\\Pr(\\text{point falls inside unit sphere}) = \\frac{V_{\\text{sphere}}}{V_{\\text{cube}}} = \\frac{4\\pi/3}{8} = \\frac{\\pi}{6}\n",
        "$$\n",
        "In a Monte Carlo experiment, we draw $N$ random sample points uniformly in the cube and count how many of them fall inside the sphere. Let $N_{\\text{inside}}$ be the number of points (out of $N$) satisfying the sphere’s equation $x^2 + y^2 + z^2 \\le 1$. By the **Law of Large Numbers**, the ratio $N_{\\text{inside}}/N$ will converge to the true probability $\\pi/6$ as $N$ becomes large. Thus, an estimator for the sphere’s volume can be obtained from the sample as:\n",
        "$$\n",
        "\\hat{V}_{\\text{sphere}} = \\frac{N_{\\text{inside}}}{N} \\times V_{\\text{cube}} = \\frac{N_{\\text{inside}}}{N} \\times 8\n",
        "$$\n",
        "This Monte Carlo estimate $\\hat V_{\\text{sphere}}$ is an approximation of the true sphere volume $4\\pi/3$. The larger the number of sample points $N$, the more accurate (on average) $\\hat V_{\\text{sphere}}$ becomes, with the estimation error decreasing on the order of $1/\\sqrt{N}$ (characteristic of Monte Carlo methods).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#### Mathematically Interesting Fact about Rejection Rate\n",
        "\n",
        "The trend observed from 2D to 3D (that the fraction of points inside the sphere decreases) continues as the number of dimensions increases. Geometrically, as the dimension $d$ grows, the volume of the inscribed $d$-dimensional ball (hypersphere) grows more slowly compared to the volume of the surrounding hypercube. In fact, for high $d$, most of the hypercube’s volume resides in the “corners” far from the center, while the hypersphere’s volume is concentrated near the center. The number of rejections — i.e. the proportion of sample points that fall outside the hypersphere — therefore increases with dimension. For a unit $d$-sphere (radius 1) inside a $[-1,1]^d$ hypercube, the volume of the hypercube is $2^d$, while the volume of the $d$-sphere is given by the formula:\n",
        "$$\n",
        "V_d(\\text{unit sphere}) = \\frac{\\pi^{d/2}}{\\Gamma\\!\\left(\\frac{d}{2} + 1\\right)}\n",
        "$$\n",
        "where $\\Gamma$ is the gamma function (which generalizes factorials). For $d=2$, this gives $V_2 = \\pi$; for $d=3$, $V_3 = 4\\pi/3$, as expected. The fraction of the hypercube’s volume occupied by the sphere is $\\frac{V_d}{2^d}$. Plugging in small dimensions:\n",
        "* In 2D: $\\frac{V_2}{2^2} = \\frac{\\pi}{4} \\approx 0.785$. (About 78.5% of the square is inside the circle.)\n",
        "* In 3D: $\\frac{V_3}{2^3} = \\frac{4\\pi/3}{8} = \\frac{\\pi}{6} \\approx 0.524$. (About 52.4% of the cube is inside the sphere.)\n",
        "\n",
        "We see that the volume fraction (and thus acceptance probability) drops rapidly as $d$ increases. By 10 dimensions, the unit sphere occupies only a vanishingly small fraction of the 10-cube (on the order of $0.1%$). For example, in a 10-dimensional Monte Carlo experiment, only about 0.25% of random points lie inside the unit 10-sphere, meaning 99.75% of samples are rejected. This dramatic decline in efficiency is a manifestation of the curse of dimensionality, wherein high-dimensional geometries become challenging for brute-force methods​. In practical terms, as dimension grows, one needs exponentially more samples to get even a few points inside the hypersphere. The Monte Carlo standard error still decreases as $1/\\sqrt{N}$, but if only a tiny fraction of $N$ points contribute (fall inside), the absolute error in volume estimation can remain large unless $N$ is extremely large. Thus, the 3D case (with about half the samples useful) is far more efficient than, say, a 10D case (with almost all samples wasted), but still less efficient than the 2D case (where a large majority of samples hit the target region)."
      ],
      "metadata": {
        "id": "BZVPI1qF2AFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    N = 10_000_000  # Number of points to sample\n",
        "    start = time.time()\n",
        "    pi_approx = sample_mc_3d(N)\n",
        "    end = time.time()\n",
        "    print(f\"3D MC Approx of pi (N={N}): {pi_approx}\")\n",
        "    print(f\"Absolute Error: {abs(pi_approx - math.pi)}\")\n",
        "    print(f\"Time Taken: {end - start:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGLMpVR2PZ5r",
        "outputId": "a6d7d9a7-4306-4f6f-c02c-464939a27a6d"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3D MC Approx of pi (N=10000000): 3.1407887999999997\n",
            "Absolute Error: 0.0008038535897934018\n",
            "Time Taken: 8.11 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3.4. 2D Gaussian\n",
        "\n",
        "We use a 2D Gaussian distribution centered at the origin $(0,0)$ as our proposal distribution $q(x,y)$. This distribution assigns higher probability density to points near the center and lower density to points farther away, which aligns well with the region of interest (the unit circle around the origin). We define:\n",
        "$$\n",
        "q(x,y) \\;=\\; \\frac{1}{2\\pi\\,\\sigma^2}\\,\\exp\\Bigl(-\\,\\frac{x^2 + y^2}{2\\,\\sigma^2}\\Bigr)\n",
        "$$\n",
        "where $\\sigma^2$ is the variance in each coordinate (the covariance matrix is $\\sigma^2 I$). This isotropic Gaussian is a valid probability density over $\\mathbb{R}^2$ (its total integral is 1). The choice of $\\sigma$ controls how spread out the samples are:\n",
        "* If $\\sigma$ is small, $q$ is tightly concentrated around $(0,0)$, so most samples will fall inside the unit circle (since the circle of radius 1 covers mostly the high-density region of $q$). However, points near the boundary of the circle will have very small $q(x,y)$ values and hence very large weights $1/q(x,y)$.\n",
        "* If $\\sigma$ is large, $q$ is more diffuse; it will sample far outside the circle as well. Many points will fall outside $A$ (contributing zero), and even points inside $A$ might have only moderate weights (since $q$ is not too peaked). This can lead to more wasted samples (those outside) and higher variance.\n",
        "By choosing a moderate $\\sigma$, we ensure $q$ places substantial probability mass inside the unit circle while still not decaying too sharply within $A$. In other words, the Gaussian proposal serves to “focus” sampling around the region $A$ without neglecting any part of it. As a result, more samples contribute meaningfully to the area estimate compared to uniform sampling. Note: It is critical that $q(x,y) > 0$ everywhere on the unit circle; the Gaussian meets this requirement since it has full support on $\\mathbb{R}^2$.\n",
        "\n",
        "To compute the area of a region $A \\subset \\mathbb{R}^2$ using any PDF $q(x,y)$, observe that\n",
        "\n",
        "$$\n",
        "\\text{Area}(A)\n",
        "\\;=\\;\n",
        "\\iint_{A} \\!1\\,d(x,y)\n",
        "\\;=\\;\n",
        "\\iint_{\\mathbb{R}^2}\n",
        "1_{A}(x,y)\\,\\frac{1}{q(x,y)}\\,q(x,y)\\,d(x,y).\n",
        "$$\n",
        "\n",
        "Thus, the area can be viewed as an expectation under the distribution $q$:\n",
        "\n",
        "$$\n",
        "\\hat{\\pi}\n",
        "\\;=\\;\n",
        "\\text{Area}(A)\n",
        "\\;=\\;\n",
        "\\mathbb{E}_{(X,Y)\\,\\sim\\,q}\n",
        "\\Bigl[\\,1_{A}(X,Y)\\,\\frac{1}{q(X,Y)}\\Bigr]\n",
        "\\;\\approx\\;\n",
        "\\frac{1}{N}\\,\\sum_{i=1}^{N}\n",
        "\\Bigl[\\,1_{A}(X_i,Y_i)\\,\\frac{1}{q(X_i,Y_i)}\\Bigr]\n",
        "$$\n",
        "\n",
        "where $(X_i,Y_i)$ are i.i.d. samples from $q$. That is, we only add the weight $1/q(X_i,Y_i)$ if the sampled point lies inside the circle, otherwise we add zero. This method is a straightforward application of **importance sampling**, ensuring that sampling from a non-uniform $q$ can still yield an unbiased estimate for the **area** of $A$.\n"
      ],
      "metadata": {
        "id": "PNhYWitG2Gfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    N = 10_000_00  # 1e7 for a better approximation; try smaller first for speed\n",
        "    sigma = 0.5\n",
        "    start = time.time()\n",
        "    pi_approx = estimate_pi_importance_gaussian(N, sigma)\n",
        "    end = time.time()\n",
        "\n",
        "    print(f\"Gaussian IS Approx of pi (N={N}, sigma={sigma}): {pi_approx}\")\n",
        "    print(f\"Absolute Error: {abs(pi_approx - math.pi)}\")\n",
        "    print(f\"Time Taken: {end - start:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LV2RLCR-Dy4I",
        "outputId": "fa27ca18-0791-488d-9216-567236ac2bc6"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian IS Approx of pi (N=1000000, sigma=0.5): 3.1398878093304994\n",
            "Absolute Error: 0.0017048442592937363\n",
            "Time Taken: 0.87 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4. Advanced Techniques for MCMC"
      ],
      "metadata": {
        "id": "qcqelEPu0U66"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4.1. Adaptive Step-Size MCMC\n",
        "\n",
        "In a Markov Chain Monte Carlo (MCMC) approach, we generate a sequence of sample points that are dependent on each other, rather than picking each point independently. The key is to design a Markov chain (a random walk in the square) whose stationary distribution is uniform on the unit square​. This ensures that, in the long run, the density of sample points produced by the chain is uniform over the square, just like independent uniform sampling. To construct such a chain, we use the Metropolis algorithm to move through the $(x,y)$ space, deciding whether to accept or reject proposed moves so as to maintain the correct distribution. At each step:\n",
        "\n",
        "1. **Initialization:**\n",
        "  * Start $(x,y)$ near the center, or any point inside $[0,1] \\times [0,1]$.\n",
        "  * Choose an initial step size, say $\\delta_0 = 0.1$.\n",
        "\n",
        "2. **Sampling Loop:**\n",
        "  * $x_{\\text{new}} = x_i + U(-\\delta_i,\\delta_i), y_{\\text{new}} = y_i + U(-\\delta_i,\\delta_i)$.\n",
        "  * If in $[0,1] \\times [0,1]$, accept it: $(x_{i+1},y_{i+1}) = (x_{\\text{new}},y_{\\text{new}})$. Otherwise, reject: $(x_{i+1},y_{i+1}) = (x_i,y_i)$.\n",
        "  * Adjust step size $\\delta_{i+1}$ based on the acceptance rate so far.\n",
        "\n",
        "3. **Count if $(x_i,y_i)$ lies in the unit circle**\n",
        "\n",
        "This procedure is repeated for many iterations, producing a Markov chain $(x_0,y_0)\\to(x_1,y_1)\\to\\cdots$ that explores the unit square. Throughout the simulation we keep track of how often the chain’s points fall inside the quarter circle (i.e. count the number of steps where $x^2+y^2 \\le 1$). After a large number of steps, the fraction of samples inside the quarter circle will be our estimate for the probability $\\Pr{(x,y)\\text{ inside quarter circle}} = \\pi$."
      ],
      "metadata": {
        "id": "6IL8muC72E6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    samples = 10_000_000\n",
        "    start_time = time.time()\n",
        "    pi_estimate = mcmc_pi_adaptive(samples)\n",
        "    end_time = time.time()\n",
        "    print(f\"Samples: {samples}, Pi ≈ {pi_estimate}, Error: {abs(pi_estimate - math.pi)}\")\n",
        "    print(f\"Time Taken: {end_time - start_time:.3f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiP7kmLCXt4b",
        "outputId": "f44efce0-4c4c-4dcd-8f07-4557db35539f"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samples: 10000000, Pi ≈ 3.1415056, Error: 8.705358979321787e-05\n",
            "Time Taken: 7.161 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4.2. Parallel Chains\n",
        "\n",
        "Each MCMC chain runs independently using a simple Metropolis algorithm to sample points uniformly within the unit square $[0,1] \\times [0,1]$. In this setup, the target distribution is uniform over the unit square, so the Metropolis-Hastings rule accepts any proposed move that stays inside the square and rejects moves that would leave the domain​. This ensures each chain individually produces a stream of $(x,y)$ points that are distributed approximately uniformly over the unit square. Crucially, the chains do not interact with each other in any way, so they remain independent in their behavior and output.\n",
        "\n",
        "To estimate $\\pi$, the algorithm leverages the geometry of a quarter circle inscribed in the unit square. In each chain, as points are sampled, we check whether they fall inside the quarter circle of radius 1 (i.e. satisfy $x^2+y^2 \\le 1$). The fraction of points that land inside this quarter circle approximates the ratio of the quarter-circle’s area to the area of the whole square​. Since the area of the full unit square is 1 and the area of the quarter circle is $\\pi/4$, this fraction should be about $\\pi/4$. Thus, an estimate for π is obtained by multiplying the inside-circle fraction by 4​. For example, if a total of $N$ points are sampled in a chain and $M$ of them fell inside the quarter circle, we compute $\\pi \\approx 4 \\times \\frac{M}{N}$. As $N$ becomes large, this Monte Carlo estimate converges to the true value of π by the law of large numbers. Because each MCMC chain operates independently and targets the same uniform distribution, we can run multiple chains in parallel without any interference or communication between them. In fact, generating random sample points is an embarrassingly parallel task: the generation of each point (or each chain’s sequence of points) is independent of all others​. This independence means that samples drawn by different chains have no correlation with each other, which avoids the issues of autocorrelation that occur between successive samples within a single chain. With independent chains, we can safely collect results from many samplers running simultaneously, effectively increasing the total number of samples gathered in a given time. In other words, running $k$ chains at once can produce roughly $k$ times more samples in the same wall-clock time, since (conditional on different random seeds) the chains are independent and can be simulated concurrently​.\n",
        "\n",
        "After all chains have run (or periodically during execution), the results from the separate chains are combined to form a global estimate of π. Since each chain counted how many of its points landed inside the quarter circle, we can sum these counts across all chains and likewise sum the total number of points sampled across all chains. If chain 1 had $M_1$ hits out of $N_1$ samples, chain 2 had $M_2$ out of $N_2$​, and so on, then overall we have $M_{\\text{total}} = M_1 + M_2 + \\cdots$ inside-circle points out of $N_{\\text{total}} = N_1 + N_2 + \\cdots$ total points. The global estimate of π is then calculated as $4 \\times \\frac{M_{\\text{total}}}{N_{\\text{total}}}$​. This procedure is equivalent to pooling all the samples together: it does not matter whether the $N_{\\text{total}}$ points came from one long chain or from several independent chains. In fact, Monte Carlo estimators are additive, so we can “add all of these together before estimating $\\pi$\" - effectively taking an average across chains. By combining the chains’ outcomes in this way, we obtain a single, more precise estimate. Each chain’s contribution improves the overall accuracy, and the uncertainty of the final estimate decreases as we aggregate more independent samples."
      ],
      "metadata": {
        "id": "AKRPuFQR2Oqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    total_samples = 10_000_000\n",
        "    num_procs = 4  # number of parallel processes (cores)\n",
        "\n",
        "    # We split total_samples across processes\n",
        "    samples_per_proc = total_samples // num_procs\n",
        "\n",
        "    start = time.time()\n",
        "    with mp.Pool(processes=num_procs) as pool:\n",
        "        # Launch parallel tasks\n",
        "        results = pool.starmap(\n",
        "            mcmc_chain,\n",
        "            [(samples_per_proc, 0.1, 42 + i) for i in range(num_procs)]  # seeds to vary\n",
        "        )\n",
        "\n",
        "    total_inside = sum(results)\n",
        "    pi_estimate = 4.0 * total_inside / (samples_per_proc * num_procs)\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "    print(f\"Parallel MCMC Approx of pi: {pi_estimate}\")\n",
        "    print(f\"Absolute Error: {abs(pi_estimate - math.pi)}\")\n",
        "    print(f\"Time Taken: {end - start:.3f} seconds using {num_procs} processes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdJtbl2TClGn",
        "outputId": "dca00b73-38d1-4fef-bfdd-47323ec58bd9"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parallel MCMC Approx of pi: 3.13761\n",
            "Absolute Error: 0.003982653589793106\n",
            "Time Taken: 6.281 seconds using 4 processes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Summary\n",
        "\n",
        "* **Leibniz Series:** While Aitken Acceleration provides the best accuracy by far, it requires more computational effort, leading to the longest execution time. Kahan Summation offers a middle ground—retaining good accuracy (near $10^{-7}$) but with notably reduced computational time compared to the basic Leibniz series implementation.\n",
        "\n",
        "* **Integration:** Both Trapezoidal and Simpson’s Rule are highly accurate for these tasks, with Simpson’s Rule edging out slightly in precision. The Monte Carlo method is the quickest but comes with a higher error magnitude unless significantly more samples are used.\n",
        "\n",
        "* **Monte Carlo:** QMC provides the highest accuracy but is computationally expensive, while Gaussian Importance Sampling is the fastest but least accurate with the chosen parameters. Stratified Sampling offers a good balance of relatively low error and moderate running time.\n",
        "\n",
        "* **MCMC:** Adaptive MCMC strikes the best balance here, achieving the highest accuracy and a shorter runtime than standard MCMC. Parallel MCMC can reduce total wall-clock time, though at the cost of higher error, depending on parameters and how the chains are managed.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Overall, mathematically driven methods (Leibniz Series and Integration) achieve very high accuracy ($10^{-11}$ to $10^{-15}$) but may require significant iteration counts, clever summation techniques, or dense subdivisions. Often more “exact” in their convergence properties but can be slower at extreme precisions. Monte Carlo methods and MCMC were faster to implement and easily parallelized, but the variance can be large; high accuracy demands a lot of samples."
      ],
      "metadata": {
        "id": "mRgIECbBzsJX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Reference\n",
        "\n",
        "[1] [Wikipedia: Leibniz Series](https://en.wikipedia.org/wiki/Leibniz_formula_for_%CF%80#:~:text=Taylor%20series%20%20for%20the,1%29%5E%7Bk%7Dx%5E%7B2k%2B1%7D%7D%7B2k%2B1)\n",
        "\n",
        "[2] [Trapezoidal Rule](https://math.libretexts.org/Courses/Community_College_of_Denver/MAT_2420_Calculus_II/03%3A_Techniques_of_Integration/3.06%3A_Numerical_Integration)\n",
        "\n",
        "[3] [Wikipedia: Monte Carlo](https://en.wikipedia.org/wiki/Monte_Carlo_method)\n",
        "\n",
        "[4] [Wikipedia: MCMC](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo)\n",
        "\n",
        "[5] [Wikipedia: Aitken Process](https://en.wikipedia.org/wiki/Aitken%27s_delta-squared_process#:~:text=squared%20process%20associates%20to%20this,sequence%20the%20new%20sequence)\n",
        "\n",
        "[6] [Aitken Formula](https://francisbach.com/acceleration-without-pain/#:~:text=%5C%28x%20%5Cmapsto%20%5Cfrac%7B1%7D%7B1%2Bx,3.14%7D27%20%5C%5C%208)\n",
        "\n",
        "[7] [Stratified Sampling](https://xuk.ai/blog/stratified-sampling.html#:~:text=1)\n",
        "\n",
        "[8] [Low Descrepency Sequence](https://en.wikipedia.org/wiki/Low-discrepancy_sequence#:~:text=Roughly%20speaking%2C%20the%20discrepancy%20of,by%20taking%20the%20worst%20value)\n",
        "\n",
        "[9] [2D Gaussian](https://builtin.com/articles/importance-sampling#:~:text=Where%20,offset%20the%20probability%20of%20sampling)\n",
        "\n",
        "[10] [Adaptive MCMC](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=503dd21a40c966736df809a96c8e31277337e59d#:~:text=A%20simple%20way%20to%20avoid,Figure%203)\n",
        "\n",
        "[11] [Parallel Chains](https://bede-documentation.readthedocs.io/en/latest/guides/wanderings/Estimating-pi-in-CUDALand.html#)"
      ],
      "metadata": {
        "id": "1SqCjOFB9B2V"
      }
    }
  ]
}